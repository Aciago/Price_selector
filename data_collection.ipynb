{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29deb1ad-5068-485d-8e44-6accd420a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ee4b57-8557-4088-ba75-ef0acfd93eff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL to scrape\n",
    "url = 'https://www.naturabuy.fr/Munitions-Balles-22LR-cat-884.html'\n",
    "\n",
    "# Send a GET request to the URL and store the response\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the response using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all the offers on the page\n",
    "offers = soup.find_all('div', class_='BlocVente')\n",
    "\n",
    "# Create an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through each offer and extract the desired information\n",
    "for offer in offers:\n",
    "    # Extract caliber name\n",
    "    cal_name = offer.find()\n",
    "    \n",
    "    # Extract manufacturers name\n",
    "    manufacturer_name = offer.find()\n",
    "    \n",
    "    # Extract the item name\n",
    "    item_name = offer.find('a', class_='TitreVente').text.strip()\n",
    "    \n",
    "    # Extract the quantity of bullets\n",
    "    quantity = offer.find('span', class_='Qte').text.strip()\n",
    "    \n",
    "    # Extract the price\n",
    "    price = offer.find('div', class_='Prix').text.strip()\n",
    "    \n",
    "    # Extract the delivery price\n",
    "    delivery_price = offer.find('div', class_='Livraison').text.strip()\n",
    "    \n",
    "    # Append the data for this offer to the list\n",
    "    data.append([item_name, quantity, price, delivery_price])\n",
    "\n",
    "# Create a pandas DataFrame from the list of data\n",
    "df = pd.DataFrame(data, columns=['Item Name', 'Quantity', 'Price', 'Delivery Price'])\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df.to_csv('ammo_prices.csv', index=False)\n",
    "\n",
    "# Print a message to indicate that the data has been saved\n",
    "print(\"Data has been saved to 'ammo_prices.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78bd4f-6d62-4087-af50-b7b67b5f8816",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the base URL and initialize a list to store the results\n",
    "base_url = 'https://www.naturabuy.fr/Munitions-Balles-22LR-cat-884.html'\n",
    "data_list = []\n",
    "\n",
    "while True:\n",
    "    # make a request to the current page\n",
    "    page = requests.get(base_url)\n",
    "    \n",
    "    # parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # find all the item cards and extract the data\n",
    "    cards = soup.find_all('div', {'class': ['itemcard', 'itemcard promo']})\n",
    "    for card in cards:\n",
    "        item_dict = {}\n",
    "        item_dict['title'] = card.find('a', {'class': 'itemLink'}).get('title')\n",
    "        item_dict['data-id'] = card.get('data-id')\n",
    "        item_dict['data-name'] = card.get('data-name')\n",
    "        item_dict['data-category'] = card.get('data-category')\n",
    "        item_dict['data-brand'] = card.get('data-brand')\n",
    "        item_dict['data-price'] = card.get('data-price')\n",
    "        item_dict['data-position'] = card.get('data-position')\n",
    "        item_dict['data-dimension1'] = card.get('data-dimension1')\n",
    "        item_dict['data-dimension4'] = card.get('data-dimension4')\n",
    "        item_dict['data-dimension5'] = card.get('data-dimension5')\n",
    "        item_dict['data-dimension6'] = card.get('data-dimension6')\n",
    "        data_list.append(item_dict)\n",
    "    \n",
    "    # find the next page button\n",
    "    next_page = soup.find('a', {'class': 'paginationElement', 'data-page': str(int(soup.find('a', {'class': 'paginationElement activePage'}).get('data-page'))+1)})\n",
    "    \n",
    "    # break the loop if there is no next page button\n",
    "    if next_page is None:\n",
    "        break\n",
    "    \n",
    "    # update the base URL to the URL of the next page\n",
    "    base_url = next_page.get('href')\n",
    "    \n",
    "# convert the data list to a pandas DataFrame and save it as a CSV file\n",
    "df = pd.DataFrame(data_list)\n",
    "df.to_csv('data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb21b704-650b-4402-9621-d4af0ff00247",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 done!\n",
      "Page 2 done!\n",
      "Empty DataFrame\n",
      "Columns: [Title, ID, Name, Category, Brand, Price, Position, Dimension1, Dimension4, Dimension5, Dimension6]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Set the number of pages and items to scrape for testing\n",
    "max_pages = 2\n",
    "max_items = 5\n",
    "\n",
    "# Set a delay in seconds to avoid being blocked while testing\n",
    "delay = 2\n",
    "\n",
    "# Create empty lists to store scraped data\n",
    "titles = []\n",
    "ids = []\n",
    "names = []\n",
    "categories = []\n",
    "brands = []\n",
    "prices = []\n",
    "positions = []\n",
    "dimensions1 = []\n",
    "dimensions4 = []\n",
    "dimensions5 = []\n",
    "dimensions6 = []\n",
    "\n",
    "# Loop through each page up to the maximum number of pages to scrape\n",
    "for i in range(1, max_pages + 1):\n",
    "    # Construct the URL for the current page\n",
    "    url = f'https://www.naturabuy.fr/Munitions-Balles-22LR-cat-884.html?PAGE={i}'\n",
    "    # Make a GET request to the page\n",
    "    response = requests.get(url)\n",
    "    # Wait for a short time to avoid being blocked while testing\n",
    "    time.sleep(delay)\n",
    "    # Parse the HTML content of the response using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # Find all the items on the current page\n",
    "    items = soup.find_all('div', {'class': ['itemcard', 'itemcard promo']})\n",
    "    # Loop through each item up to the maximum number of items to scrape\n",
    "    for j, item in enumerate(items[:max_items]):\n",
    "        # Scrape the desired data from the current item\n",
    "        title = item.find('a', {'class': 'itemTitle'}).get('title')\n",
    "        data_id = item.get('data-id')\n",
    "        data_name = item.get('data-name')\n",
    "        data_category = item.get('data-category')\n",
    "        data_brand = item.get('data-brand')\n",
    "        data_price = item.get('data-price')\n",
    "        data_position = item.get('data-position')\n",
    "        data_dimension1 = item.get('data-dimension1')\n",
    "        data_dimension4 = item.get('data-dimension4')\n",
    "        data_dimension5 = item.get('data-dimension5')\n",
    "        data_dimension6 = item.get('data-dimension6')\n",
    "        # Add the scraped data to the corresponding lists\n",
    "        titles.append(title)\n",
    "        ids.append(data_id)\n",
    "        names.append(data_name)\n",
    "        categories.append(data_category)\n",
    "        brands.append(data_brand)\n",
    "        prices.append(data_price)\n",
    "        positions.append(data_position)\n",
    "        dimensions1.append(data_dimension1)\n",
    "        dimensions4.append(data_dimension4)\n",
    "        dimensions5.append(data_dimension5)\n",
    "        dimensions6.append(data_dimension6)\n",
    "    # Print a message to indicate that the current page has been scraped\n",
    "    print(f'Page {i} done!')\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "df = pd.DataFrame({\n",
    "    'Title': titles,\n",
    "    'ID': ids,\n",
    "    'Name': names,\n",
    "    'Category': categories,\n",
    "    'Brand': brands,\n",
    "    'Price': prices,\n",
    "    'Position': positions,\n",
    "    'Dimension1': dimensions1,\n",
    "    'Dimension4': dimensions4,\n",
    "    'Dimension5': dimensions5,\n",
    "    'Dimension6': dimensions6\n",
    "})\n",
    "\n",
    "# Print the DataFrame to verify that the data has been scraped correctly\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53eadd23-b661-4e18-9d4f-53a5d609d751",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aciag\\AppData\\Local\\Temp\\ipykernel_6888\\3654172229.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.Series([title, data_id, data_name, data_category, data_brand, data_price, data_position, data_dimension1, data_dimension4, data_dimension5, data_dimension6], index=columns), ignore_index=True)\n",
      "C:\\Users\\aciag\\AppData\\Local\\Temp\\ipykernel_6888\\3654172229.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.Series([title, data_id, data_name, data_category, data_brand, data_price, data_position, data_dimension1, data_dimension4, data_dimension5, data_dimension6], index=columns), ignore_index=True)\n",
      "C:\\Users\\aciag\\AppData\\Local\\Temp\\ipykernel_6888\\3654172229.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.Series([title, data_id, data_name, data_category, data_brand, data_price, data_position, data_dimension1, data_dimension4, data_dimension5, data_dimension6], index=columns), ignore_index=True)\n",
      "C:\\Users\\aciag\\AppData\\Local\\Temp\\ipykernel_6888\\3654172229.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.Series([title, data_id, data_name, data_category, data_brand, data_price, data_position, data_dimension1, data_dimension4, data_dimension5, data_dimension6], index=columns), ignore_index=True)\n",
      "C:\\Users\\aciag\\AppData\\Local\\Temp\\ipykernel_6888\\3654172229.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.Series([title, data_id, data_name, data_category, data_brand, data_price, data_position, data_dimension1, data_dimension4, data_dimension5, data_dimension6], index=columns), ignore_index=True)\n",
      "C:\\Users\\aciag\\AppData\\Local\\Temp\\ipykernel_6888\\3654172229.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.Series([title, data_id, data_name, data_category, data_brand, data_price, data_position, data_dimension1, data_dimension4, data_dimension5, data_dimension6], index=columns), ignore_index=True)\n",
      "C:\\Users\\aciag\\AppData\\Local\\Temp\\ipykernel_6888\\3654172229.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.Series([title, data_id, data_name, data_category, data_brand, data_price, data_position, data_dimension1, data_dimension4, data_dimension5, data_dimension6], index=columns), ignore_index=True)\n",
      "C:\\Users\\aciag\\AppData\\Local\\Temp\\ipykernel_6888\\3654172229.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.Series([title, data_id, data_name, data_category, data_brand, data_price, data_position, data_dimension1, data_dimension4, data_dimension5, data_dimension6], index=columns), ignore_index=True)\n",
      "C:\\Users\\aciag\\AppData\\Local\\Temp\\ipykernel_6888\\3654172229.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.Series([title, data_id, data_name, data_category, data_brand, data_price, data_position, data_dimension1, data_dimension4, data_dimension5, data_dimension6], index=columns), ignore_index=True)\n",
      "C:\\Users\\aciag\\AppData\\Local\\Temp\\ipykernel_6888\\3654172229.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.Series([title, data_id, data_name, data_category, data_brand, data_price, data_position, data_dimension1, data_dimension4, data_dimension5, data_dimension6], index=columns), ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title        id  \\\n",
      "0               500 MUNITIONS RWS 22LR Z LANG 29 GR   10285119   \n",
      "1  Cartouches FEDERAL - cal. 22LR - 36gr CPHP Hol...   9472173   \n",
      "2  500 Munitions Remington 22 lr cuivrée VIPER HV...  10190718   \n",
      "3                             Balle 22 lr winchester  10285082   \n",
      "4  500 Munitions CCI Standard Velocity Calibre 22...   6183374   \n",
      "5  Balles RWS Pistol Match - Cal. 22LR - 22LR / P...   9273690   \n",
      "6  Balles RWS Pistol Match SR - Cal. 22LR - 22LR ...   9273691   \n",
      "7   Balles RWS club - Cal. 22LR - 22LR / Par 10 / 40   9273683   \n",
      "8   Balles RWS club - Cal. 22LR - 22LR / Par 20 / 40   9273684   \n",
      "9  Balles RWS High Velocity - Cal. 22LR - 22LR / ...   9273685   \n",
      "\n",
      "                                                name  \\\n",
      "0               500 MUNITIONS RWS 22LR Z LANG 29 GR    \n",
      "1  Cartouches FEDERAL - cal. 22LR - 36gr CPHP Hol...   \n",
      "2  500 Munitions Remington 22 lr cuivrée VIPER HV...   \n",
      "3                             Balle 22 lr winchester   \n",
      "4  500 Munitions CCI Standard Velocity Calibre 22...   \n",
      "5  Balles RWS Pistol Match - Cal. 22LR - 22LR / P...   \n",
      "6  Balles RWS Pistol Match SR - Cal. 22LR - 22LR ...   \n",
      "7   Balles RWS club - Cal. 22LR - 22LR / Par 10 / 40   \n",
      "8   Balles RWS club - Cal. 22LR - 22LR / Par 20 / 40   \n",
      "9  Balles RWS High Velocity - Cal. 22LR - 22LR / ...   \n",
      "\n",
      "                                            category       brand price  \\\n",
      "0  Munitions et accessoires/Munitions tir de lois...         RWS  67.3   \n",
      "1  Munitions et accessoires/Munitions tir de lois...     Federal   209   \n",
      "2  Munitions et accessoires/Munitions tir de lois...   Remington  74.8   \n",
      "3  Munitions et accessoires/Munitions tir de lois...  Winchester     6   \n",
      "4  Munitions et accessoires/Munitions tir de lois...         CCI  64.9   \n",
      "5  Munitions et accessoires/Munitions tir de lois...         RWS   138   \n",
      "6  Munitions et accessoires/Munitions tir de lois...         RWS    71   \n",
      "7  Munitions et accessoires/Munitions tir de lois...         RWS    49   \n",
      "8  Munitions et accessoires/Munitions tir de lois...         RWS    98   \n",
      "9  Munitions et accessoires/Munitions tir de lois...         RWS    72   \n",
      "\n",
      "  position dimension1 dimension4 dimension5    dimension6  \n",
      "0        1                  neuf     211758  professionel  \n",
      "1        2                  neuf     508191  professionel  \n",
      "2        3                  neuf     211758  professionel  \n",
      "3        4                  neuf     522734   particulier  \n",
      "4        5                  neuf     464111  professionel  \n",
      "5        1                  neuf     562611  professionel  \n",
      "6        2                  neuf     562611  professionel  \n",
      "7        3                  neuf     562611  professionel  \n",
      "8        4                  neuf     562611  professionel  \n",
      "9        5                  neuf     562611  professionel  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set headers to avoid being blocked\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "# Define the URL template and number of pages to scrape for testing\n",
    "url_template = 'https://www.naturabuy.fr/Munitions-Balles-22LR-cat-884.html?PAGE={}'\n",
    "max_pages = 2\n",
    "\n",
    "# Create an empty dataframe to store the scraped data\n",
    "columns = ['title', 'id', 'name', 'category', 'brand', 'price', 'position', 'dimension1', 'dimension4', 'dimension5', 'dimension6']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Loop through the specified number of pages\n",
    "for page_num in range(1, max_pages+1):\n",
    "    # Construct the URL for the current page\n",
    "    url = url_template.format(page_num)\n",
    "    \n",
    "    # Make the request and parse the HTML response\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all the item cards on the page\n",
    "    items = soup.select('html body div#contall div#body_container div#body_container_in div#PAGE form#formCritere.nbForm div#Columns2 div#Column div#auctionlist div#storelist a.itemcard, \\\n",
    "                         html body div#contall div#body_container div#body_container_in div#PAGE form#formCritere.nbForm div#Columns2 div#Column div#auctionlist div#storelist a.itemcard.promo')\n",
    "    \n",
    "    # Extract the data from each item card and append it to the dataframe\n",
    "    for item in items[:5]:\n",
    "        title = item['title']\n",
    "        data_id = item['data-id']\n",
    "        data_name = item['data-name']\n",
    "        data_category = item['data-category']\n",
    "        data_brand = item['data-brand']\n",
    "        data_price = item['data-price']\n",
    "        data_position = item['data-position']\n",
    "        data_dimension1 = item['data-dimension1']\n",
    "        item_new = item['data-dimension4']\n",
    "        data_dimension5 = item['data-dimension5']\n",
    "        vendor = item['data-dimension6']\n",
    "        \n",
    "        df = df.append(pd.Series([title, data_id, data_name, data_category, data_brand, data_price, data_position, data_dimension1, data_dimension4, data_dimension5, data_dimension6], index=columns), ignore_index=True)\n",
    "    \n",
    "    # Add a delay before making the next request to avoid being blocked\n",
    "    time.sleep(2)\n",
    "\n",
    "# Print the resulting dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7508889c-bf07-49a3-8624-848c588bf325",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title   data-id  \\\n",
      "0               500 munitions RWS 22 lr SUBSONIC HP    9686142   \n",
      "1  500 munitions RWS Rifle Match Professional Lin...   9686088   \n",
      "2  BOITE DE 50 MUNITIONS RWS MODEL pistolet MATCH...   4818646   \n",
      "3     500 munitions RWS Pistol Match calibre .22 LR    9661481   \n",
      "4               500 MUNITIONS RWS 22LR Z LANG 29 GR   10285119   \n",
      "\n",
      "                                           data-name  \\\n",
      "0               500 munitions RWS 22 lr SUBSONIC HP    \n",
      "1  500 munitions RWS Rifle Match Professional Lin...   \n",
      "2  BOITE DE 50 MUNITIONS RWS MODEL pistolet MATCH...   \n",
      "3     500 munitions RWS Pistol Match calibre .22 LR    \n",
      "4               500 MUNITIONS RWS 22LR Z LANG 29 GR    \n",
      "\n",
      "                                       data-category data-brand data-price  \\\n",
      "0  Munitions et accessoires/Munitions tir de lois...        RWS       79.8   \n",
      "1  Munitions et accessoires/Munitions tir de lois...        RWS       74.8   \n",
      "2  Munitions et accessoires/Munitions tir de lois...        RWS        7.8   \n",
      "3  Munitions et accessoires/Munitions tir de lois...        RWS       74.8   \n",
      "4  Munitions et accessoires/Munitions tir de lois...        RWS       67.3   \n",
      "\n",
      "  data-position data-dimension1 data-dimension4 data-dimension5  \\\n",
      "0             1                            neuf          211758   \n",
      "1             2                            neuf          211758   \n",
      "2             3                            neuf            1458   \n",
      "3             4                            neuf          211758   \n",
      "4             5                            neuf          211758   \n",
      "\n",
      "  data-dimension6  \n",
      "0    professionel  \n",
      "1    professionel  \n",
      "2    professionel  \n",
      "3    professionel  \n",
      "4    professionel  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Function to scrape data from a single page\n",
    "def scrape_page(url):\n",
    "    # Send GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all items on the page\n",
    "    items = soup.select('html body div#contall div#body_container div#body_container_in div#PAGE form#formCritere.nbForm div#Columns2 div#Column div#auctionlist div#storelist a.itemcard, html body div#contall div#body_container div#body_container_in div#PAGE form#formCritere.nbForm div#Columns2 div#Column div#auctionlist div#storelist a.itemcard.promo')\n",
    "    \n",
    "    # Extract data from each item\n",
    "    data = []\n",
    "    for item in items[:5]:\n",
    "        item_data = {}\n",
    "        item_data['title'] = item.get('title')\n",
    "        item_data['data-id'] = item.get('data-id')\n",
    "        item_data['data-name'] = item.get('data-name')\n",
    "        item_data['data-category'] = item.get('data-category')\n",
    "        item_data['data-brand'] = item.get('data-brand')\n",
    "        item_data['data-price'] = item.get('data-price')\n",
    "        item_data['data-position'] = item.get('data-position')\n",
    "        item_data['data-dimension1'] = item.get('data-dimension1')\n",
    "        item_data['data-dimension4'] = item.get('data-dimension4')\n",
    "        item_data['data-dimension5'] = item.get('data-dimension5')\n",
    "        item_data['data-dimension6'] = item.get('data-dimension6')\n",
    "        data.append(item_data)\n",
    "    \n",
    "    # Wait for a short time to avoid being blocked\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Return extracted data\n",
    "    return data\n",
    "\n",
    "\n",
    "# Function to scrape data from multiple pages\n",
    "def scrape_pages(start_url, num_pages):\n",
    "    # Initialize an empty DataFrame to store all data\n",
    "    all_data = pd.DataFrame()\n",
    "    \n",
    "    # Loop over the specified number of pages\n",
    "    for i in range(num_pages):\n",
    "        # Construct URL for the current page\n",
    "        if i == 0:\n",
    "            url = start_url\n",
    "        else:\n",
    "            url = start_url + f'?PAGE={i+1}'\n",
    "        \n",
    "        # Scrape data from the current page\n",
    "        data = scrape_page(url)\n",
    "        \n",
    "        # Convert the list of dictionaries to a DataFrame\n",
    "        page_data = pd.DataFrame(data)\n",
    "        \n",
    "        # Append the data to the overall DataFrame\n",
    "        all_data = pd.concat([all_data, page_data], ignore_index=True)\n",
    "    \n",
    "    # Return the overall DataFrame\n",
    "    return all_data\n",
    "\n",
    "\n",
    "# Test the code by scraping the first 2 pages\n",
    "start_url = 'https://www.naturabuy.fr/Munitions-Balles-22LR-cat-884.html'\n",
    "num_pages = 2\n",
    "data = scrape_pages(start_url, num_pages)\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb9fb6b1-39eb-43e5-906b-10a1d1744a91",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL '22LR-TARGET-RIFLE-item-9740958.html': No scheme supplied. Perhaps you meant https://22LR-TARGET-RIFLE-item-9740958.html?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# loop through the individual product pages and extract desired information\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m product_url \u001b[38;5;129;01min\u001b[39;00m product_urls:\n\u001b[1;32m---> 13\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(product_url)\n\u001b[0;32m     14\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# extract product name\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\sessions.py:573\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;66;03m# Create the Request.\u001b[39;00m\n\u001b[0;32m    561\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(\n\u001b[0;32m    562\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[0;32m    563\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    571\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[0;32m    572\u001b[0m )\n\u001b[1;32m--> 573\u001b[0m prep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_request(req)\n\u001b[0;32m    575\u001b[0m proxies \u001b[38;5;241m=\u001b[39m proxies \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    577\u001b[0m settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_environment_settings(\n\u001b[0;32m    578\u001b[0m     prep\u001b[38;5;241m.\u001b[39murl, proxies, stream, verify, cert\n\u001b[0;32m    579\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\sessions.py:484\u001b[0m, in \u001b[0;36mSession.prepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    481\u001b[0m     auth \u001b[38;5;241m=\u001b[39m get_netrc_auth(request\u001b[38;5;241m.\u001b[39murl)\n\u001b[0;32m    483\u001b[0m p \u001b[38;5;241m=\u001b[39m PreparedRequest()\n\u001b[1;32m--> 484\u001b[0m p\u001b[38;5;241m.\u001b[39mprepare(\n\u001b[0;32m    485\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[0;32m    486\u001b[0m     url\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl,\n\u001b[0;32m    487\u001b[0m     files\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mfiles,\n\u001b[0;32m    488\u001b[0m     data\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mdata,\n\u001b[0;32m    489\u001b[0m     json\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mjson,\n\u001b[0;32m    490\u001b[0m     headers\u001b[38;5;241m=\u001b[39mmerge_setting(\n\u001b[0;32m    491\u001b[0m         request\u001b[38;5;241m.\u001b[39mheaders, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, dict_class\u001b[38;5;241m=\u001b[39mCaseInsensitiveDict\n\u001b[0;32m    492\u001b[0m     ),\n\u001b[0;32m    493\u001b[0m     params\u001b[38;5;241m=\u001b[39mmerge_setting(request\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams),\n\u001b[0;32m    494\u001b[0m     auth\u001b[38;5;241m=\u001b[39mmerge_setting(auth, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth),\n\u001b[0;32m    495\u001b[0m     cookies\u001b[38;5;241m=\u001b[39mmerged_cookies,\n\u001b[0;32m    496\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mmerge_hooks(request\u001b[38;5;241m.\u001b[39mhooks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks),\n\u001b[0;32m    497\u001b[0m )\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\models.py:368\u001b[0m, in \u001b[0;36mPreparedRequest.prepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;124;03m\"\"\"Prepares the entire request with the given parameters.\"\"\"\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_method(method)\n\u001b[1;32m--> 368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_url(url, params)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_headers(headers)\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_cookies(cookies)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\models.py:439\u001b[0m, in \u001b[0;36mPreparedRequest.prepare_url\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidURL(\u001b[38;5;241m*\u001b[39me\u001b[38;5;241m.\u001b[39margs)\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheme:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingSchema(\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m: No scheme supplied. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerhaps you meant https://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    442\u001b[0m     )\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m host:\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidURL(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m: No host supplied\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mMissingSchema\u001b[0m: Invalid URL '22LR-TARGET-RIFLE-item-9740958.html': No scheme supplied. Perhaps you meant https://22LR-TARGET-RIFLE-item-9740958.html?"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# send request to the page with all products and get the links to individual product pages\n",
    "url = 'https://www.naturabuy.fr/Munitions-Balles-22LR-cat-884.html'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "product_links = soup.select('div#storelist a.itemcard, div#storelist a.itemcard.promo')\n",
    "product_urls = [link['href'] for link in product_links]\n",
    "\n",
    "# loop through the individual product pages and extract desired information\n",
    "for product_url in product_urls:\n",
    "    response = requests.get(product_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # extract product name\n",
    "    product_name = soup.select_one('div#productWrapper div#newinfos div h1#title').text.strip()\n",
    "    \n",
    "    # extract new or used\n",
    "    new_or_used = soup.select_one('div#availabilityWrapper span#availabilityCondition').text.strip()\n",
    "    \n",
    "    # extract vendor type\n",
    "    vendor_type = soup.select_one('div#availabilityWrapper div#isProPart').text.strip()\n",
    "    \n",
    "    # extract price\n",
    "    price = soup.select_one('div#bnPrices div#priceContainer.price').text.strip()\n",
    "    \n",
    "    # extract shipping cost\n",
    "    shipping_cost = soup.select_one('div#shippingsContainer b').text.strip()\n",
    "    \n",
    "    # extract website product id\n",
    "    product_id = soup.select_one('div#infoscomps div.grey div').text.strip()\n",
    "    \n",
    "    # extract item brand\n",
    "    item_brand = soup.select_one('h2#669 span.critValue').text.strip()\n",
    "    \n",
    "    # extract item description\n",
    "    item_description = soup.select_one('div#ItemDescription').text.strip()\n",
    "    \n",
    "    # print the extracted information for each product\n",
    "    print('Product Name:', product_name)\n",
    "    print('New or Used:', new_or_used)\n",
    "    print('Vendor Type:', vendor_type)\n",
    "    print('Price:', price)\n",
    "    print('Shipping Cost:', shipping_cost)\n",
    "    print('Product ID:', product_id)\n",
    "    print('Item Brand:', item_brand)\n",
    "    print('Item Description:', item_description)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea66b86d-3bdd-40a5-9a82-2b9093d90b7b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m     product_list \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml body div#contall div#body_container div#body_container_in div#PAGE form#formCritere.nbForm div#Columns2 div#Column div#auctionlist div#storelist a.itemcard, html body div#contall div#body_container div#body_container_in div#PAGE form#formCritere.nbForm div#Columns2 div#Column div#auctionlist div#storelist a.itemcard.promo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m product \u001b[38;5;129;01min\u001b[39;00m product_list:\n\u001b[1;32m---> 14\u001b[0m         link \u001b[38;5;241m=\u001b[39m product\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     15\u001b[0m         product_urls\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.naturabuy.fr/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m link)\n\u001b[0;32m     17\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "base_url = 'https://www.naturabuy.fr/Munitions-Balles-22LR-cat-884.html'\n",
    "product_urls = []\n",
    "for page_number in range(1, 3):\n",
    "    url = f\"{base_url}?PAGE={page_number}\"\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    product_list = soup.select('div', {'class': 'catResultItemTop'})\n",
    "    for product in product_list:\n",
    "        link = product.find('a')['href']\n",
    "        product_urls.append(\"https://www.naturabuy.fr/\" + link)\n",
    "\n",
    "data = []\n",
    "for url in product_urls:\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    product_name = soup.find('h1', {'class': 'produitName'}).text.strip()\n",
    "    try:\n",
    "        brand = soup.find('div', {'class': 'marque'}).text.strip()\n",
    "    except AttributeError:\n",
    "        brand = None\n",
    "    try:\n",
    "        calibre = soup.find('div', {'class': 'calibre'}).text.strip()\n",
    "    except AttributeError:\n",
    "        calibre = None\n",
    "    try:\n",
    "        condition = soup.find('div', {'class': 'etatObjet'}).text.strip()\n",
    "    except AttributeError:\n",
    "        condition = None\n",
    "    try:\n",
    "        price = soup.find('span', {'class': 'prix'}).text.strip()\n",
    "    except AttributeError:\n",
    "        price = None\n",
    "\n",
    "    data.append({\n",
    "        'Product Name': product_name,\n",
    "        'Brand': brand,\n",
    "        'Calibre': calibre,\n",
    "        'Condition': condition,\n",
    "        'Price': price\n",
    "    })\n",
    "\n",
    "    time.sleep(1) # wait for 1 second to avoid being blocked\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('naturabuy_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c94728c4-b4e7-4d09-a0cc-5db293b34c77",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Name Price Description Seller  \\\n",
      "0  None  None        None   None   \n",
      "1  None  None        None   None   \n",
      "2  None  None        None   None   \n",
      "3  None  None        None   None   \n",
      "4  None  None        None   None   \n",
      "\n",
      "                                                 URL  \n",
      "0  https://www.naturabuy.fr/22LR-TARGET-RIFLE-ite...  \n",
      "1  https://www.naturabuy.fr/500-munitions-RWS-22-...  \n",
      "2  https://www.naturabuy.fr/500-munitions-RWS-Rif...  \n",
      "3  https://www.naturabuy.fr/BOITE-DE-50-MUNITIONS...  \n",
      "4  https://www.naturabuy.fr/500-munitions-RWS-Pis...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "product_list = []\n",
    "\n",
    "# scrape first 10 pages of all products\n",
    "for page in range(1, 11):\n",
    "    url = f\"https://www.naturabuy.fr/Munitions-Balles-22LR-cat-884.html?PAGE={page}\"\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "    products = soup.select(\"html body div#contall div#body_container div#body_container_in div#PAGE form#formCritere.nbForm div#Columns2 div#Column div#auctionlist div#storelist a.itemcard, html body div#contall div#body_container div#body_container_in div#PAGE form#formCritere.nbForm div#Columns2 div#Column div#auctionlist div#storelist a.itemcard.promo\")\n",
    "\n",
    "    for product in products[:5]: # scrape first 5 items on each page\n",
    "        product_url = product[\"href\"]\n",
    "        if not product_url.startswith(\"https://www.naturabuy.fr/\"):\n",
    "            product_url = \"https://www.naturabuy.fr/\" + product_url\n",
    "        product_res = requests.get(product_url)\n",
    "        product_soup = BeautifulSoup(product_res.content, \"html.parser\")\n",
    "        try:\n",
    "            product_name = product_soup.find(\"h1\", class_=\"Title\").text.strip()\n",
    "        except AttributeError:\n",
    "            product_name = None\n",
    "        try:\n",
    "            product_price = product_soup.find(\"div\", class_=\"price-details\").text.strip()\n",
    "        except AttributeError:\n",
    "            product_price = None\n",
    "        try:\n",
    "            product_description = product_soup.find(\"div\", class_=\"Description\").text.strip()\n",
    "        except AttributeError:\n",
    "            product_description = None\n",
    "        try:\n",
    "            product_seller = product_soup.find(\"div\", class_=\"storeName\").text.strip()\n",
    "        except AttributeError:\n",
    "            product_seller = None\n",
    "        product_list.append({\n",
    "            \"Name\": product_name,\n",
    "            \"Price\": product_price,\n",
    "            \"Description\": product_description,\n",
    "            \"Seller\": product_seller,\n",
    "            \"URL\": product_url\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(product_list)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9033263-d4ac-4541-8591-2cf0ce572727",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Is New</th>\n",
       "      <th>Price</th>\n",
       "      <th>Shipping Cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N/A</td>\n",
       "      <td>Neuf</td>\n",
       "      <td>14,90 €</td>\n",
       "      <td>5,00 €</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N/A</td>\n",
       "      <td>Neuf</td>\n",
       "      <td>9,20 €</td>\n",
       "      <td>14,90 €</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N/A</td>\n",
       "      <td>Neuf</td>\n",
       "      <td>14,20 €</td>\n",
       "      <td>14,90 €</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N/A</td>\n",
       "      <td>Neuf</td>\n",
       "      <td>6,15 €</td>\n",
       "      <td>10,99 €</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N/A</td>\n",
       "      <td>Neuf</td>\n",
       "      <td>79,80 €</td>\n",
       "      <td>16,80 €</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Product Name Is New    Price Shipping Cost\n",
       "0          N/A   Neuf  14,90 €        5,00 €\n",
       "1          N/A   Neuf   9,20 €       14,90 €\n",
       "2          N/A   Neuf  14,20 €       14,90 €\n",
       "3          N/A   Neuf   6,15 €       10,99 €\n",
       "4          N/A   Neuf  79,80 €       16,80 €"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define the URL to scrape\n",
    "base_url = 'https://www.naturabuy.fr/Munitions-Balles-22LR-cat-884.html'\n",
    "page_number = 1\n",
    "\n",
    "# Create an empty list to store the scraped data\n",
    "data = []\n",
    "\n",
    "# Loop through the first two pages of the website\n",
    "while page_number <= 2:\n",
    "\n",
    "    # Construct the URL for the current page\n",
    "    url = base_url + f'?PAGE={page_number}'\n",
    "\n",
    "    # Make a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the response using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all the item cards on the page\n",
    "    cards = soup.find_all('a', class_='itemcard')\n",
    "\n",
    "    # Loop through the item cards and scrape the information\n",
    "    for card in cards[:5]:\n",
    "\n",
    "        # Get the href attribute of the item card and construct the URL for the product page\n",
    "        product_url = 'https://www.naturabuy.fr/' + card['href'].lstrip('/')\n",
    "\n",
    "        # Make a GET request to the product page\n",
    "        response = requests.get(product_url)\n",
    "\n",
    "        # Parse the HTML content of the response using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Scrape the product name\n",
    "        try:\n",
    "            product_name = soup.find('h1', class_='title').text.strip()\n",
    "        except:\n",
    "            product_name = 'N/A'\n",
    "\n",
    "        # Scrape whether the item is new or used\n",
    "        try:\n",
    "            item_is_new = soup.find('span', id='availabilityCondition').text.strip()\n",
    "        except:\n",
    "            item_is_new = 'N/A'\n",
    "\n",
    "        # Scrape the price\n",
    "        try:\n",
    "            price = soup.find('div', id='priceContainer').text.strip()\n",
    "        except:\n",
    "            price = 'N/A'\n",
    "\n",
    "        # Scrape the shipping cost\n",
    "        try:\n",
    "            shipping_cost = soup.find('div', id='shippingsContainer').find('b').text.strip()\n",
    "        except:\n",
    "            shipping_cost = 'N/A'\n",
    "\n",
    "        # Add the scraped data to the list\n",
    "        data.append({\n",
    "            'Product Name': product_name,\n",
    "            'Is New': item_is_new,\n",
    "            'Price': price,\n",
    "            'Shipping Cost': shipping_cost\n",
    "        })\n",
    "\n",
    "        # Wait for a short time to avoid getting blocked\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Increment the page number\n",
    "    page_number += 1\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame and save it to a CSV file\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('naturabuy_scraped_data.csv', index=False)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce15b3d5-971f-4f29-bd03-ef3d63af67e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
